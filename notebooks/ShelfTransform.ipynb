{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CAGpti6-jCN",
        "outputId": "9d9f8c51-62e6-45bf-8a83-ebd0748c4412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.3-bin-hadoop3.tgz\n",
        "!pip install -q pyspark pymongo python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "809491e3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\"\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date\n",
        "from pymongo import MongoClient\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"ShelfTransform\").config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
        "\n",
        "# Add your .env keys manually here (don't save in notebook)\n",
        "MONGO_URI = \"mongodb+srv://priyanshu:Piyu%40best123@cluster01.mmq51.mongodb.net/shelfsense?retryWrites=true&w=majority\"  # Paste from .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMpVT8VpCVR_"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "sales_df = spark.read.csv('sales_perishables.csv', header=True, inferSchema=True)\n",
        "inventory_df = spark.read.csv('inventory_chunk_*.csv', header=True, inferSchema=True)  # Wildcard for chunks\n",
        "weather_df = spark.read.csv('weather_daily_clean.csv', header=True, inferSchema=True)\n",
        "promo_df = spark.read.csv('promotions_weekly.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Clean dates (adjust formats if errors)\n",
        "sales_df = sales_df.withColumn('Date_Received', to_date(col('Date_Received'), 'MM/dd/yyyy'))\n",
        "inventory_df = inventory_df.withColumn('Date', to_date(col('Date'), 'yyyy-MM-dd'))\n",
        "weather_df = weather_df.withColumn('date', to_date(col('date'), 'yyyy-MM-dd'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20be6a78"
      },
      "outputs": [],
      "source": [
        "# 1. Fix column names\n",
        "inventory_df = inventory_df.withColumnRenamed(\"Product ID\", \"Product_ID\") \\\n",
        "                           .withColumnRenamed(\"Date\", \"Date_Received\")\n",
        "\n",
        "# 2. Use a REAL common key (use row number as dummy key for demo)\n",
        "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create row_id on each DataFrame before joining\n",
        "sales_df_with_id = sales_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"Product_ID\")))\n",
        "inventory_df_with_id = inventory_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"Product_ID\")))\n",
        "weather_df_with_id = weather_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"date\")))\n",
        "\n",
        "\n",
        "# 3. Re-run your join\n",
        "joined_df = (sales_df_with_id.join(inventory_df_with_id, \"row_id\", 'inner')\n",
        "                     .join(weather_df_with_id, \"row_id\", 'left')\n",
        "                     .select(sales_df_with_id['Product_ID'], sales_df_with_id['Category'], col('Stock_Quantity'), col('Units Sold').alias('Units_Sold'),\n",
        "                             col('meantemp'), col('humidity'), sales_df_with_id['Date_Received']))\n",
        "\n",
        "fact_inventory = joined_df.select('Product_ID', 'Date_Received', 'Stock_Quantity',\n",
        "                                  'Units_Sold', col('meantemp').alias('Weather_Temp'),\n",
        "                                  col('humidity').alias('Weather_Humidity'))\n",
        "\n",
        "fact_inventory.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUbeyzuPmeWX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Mock prediction: Add 'predicted_waste_risk' column (high if temp > 25 and humidity > 80)\n",
        "fact_inventory = fact_inventory.withColumn(\n",
        "    \"predicted_waste_risk\",\n",
        "    when((col(\"Weather_Temp\") > 25) & (col(\"Weather_Humidity\") > 80), \"High\")\n",
        "    .when((col(\"Weather_Temp\") > 20) & (col(\"Weather_Humidity\") > 70), \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "fact_inventory.show(5)  # Preview with new column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZSVMc4jnLgP"
      },
      "outputs": [],
      "source": [
        "import pulp\n",
        "\n",
        "# Convert to Pandas for PuLP (small data OK)\n",
        "fact_pd = fact_inventory.toPandas()\n",
        "\n",
        "# Group by Product_ID for averages\n",
        "grouped = fact_pd.groupby('Product_ID').agg({\n",
        "    'Stock_Quantity': 'mean',\n",
        "    'Units_Sold': 'mean',\n",
        "    'Weather_Temp': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# PuLP model: Minimize cost (reorder + waste risk proxy)\n",
        "prob = pulp.LpProblem(\"ReorderOptWithPrediction\", pulp.LpMinimize)\n",
        "\n",
        "products = grouped['Product_ID'].tolist()\n",
        "reorder_qty = pulp.LpVariable.dicts(\"Reorder\", products, lowBound=0, cat='Integer')\n",
        "\n",
        "# Objective: Min reorders weighted by temp risk (higher temp = higher \"waste cost\")\n",
        "prob += pulp.lpSum([reorder_qty[p] * grouped[grouped['Product_ID'] == p]['Weather_Temp'].values[0] for p in products])\n",
        "\n",
        "# Constraints: Stock + reorder >= demand (Units_Sold)\n",
        "for p in products:\n",
        "    row = grouped[grouped['Product_ID'] == p]\n",
        "    current_stock = row['Stock_Quantity'].values[0]\n",
        "    demand = row['Units_Sold'].values[0]\n",
        "    prob += reorder_qty[p] + current_stock >= demand, f\"Demand_{p}\"\n",
        "\n",
        "prob.solve()\n",
        "\n",
        "# Add results to table\n",
        "reorders = {p: pulp.value(reorder_qty[p]) for p in products}\n",
        "grouped['recommended_reorder'] = grouped['Product_ID'].map(reorders)\n",
        "grouped.to_csv('optimized_reorders.csv', index=False)  # Download this\n",
        "\n",
        "print(\"Optimized table:\")\n",
        "print(grouped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qoro09cbotGp",
        "outputId": "0495e9e4-0e78-4cfc-a376-ef09107198c1"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_6aa81a60-ad02-4334-808b-a6104f4b210e\", \"result.json\", 194178)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === COLAB ONLY — RUN ONCE ===\n",
        "import json, base64, os\n",
        "from google.colab import files\n",
        "from pyspark.sql.functions import date_format\n",
        "\n",
        "# Save outputs\n",
        "# Convert date column to string before converting to Pandas for Parquet\n",
        "fact_inventory.withColumn(\"Date_Received\", date_format(\"Date_Received\", \"yyyy-MM-dd\")).toPandas().to_parquet(\"fact_inventory.parquet\", index=False)\n",
        "\n",
        "optimized = grouped  # from your PuLP cell\n",
        "optimized.to_parquet(\"optimized_reorders.parquet\", index=False)\n",
        "\n",
        "# Create a single JSON with both\n",
        "# Convert date column to string in fact_inventory_for_json\n",
        "fact_inventory_for_json = fact_inventory.withColumn(\"Date_Received\", date_format(\"Date_Received\", \"yyyy-MM-dd\")).toPandas()\n",
        "optimized_for_json = optimized.to_dict('records')\n",
        "\n",
        "\n",
        "payload = {\n",
        "    \"fact\": fact_inventory_for_json.to_dict('records'),\n",
        "    \"optimized\": optimized_for_json\n",
        "}\n",
        "with open(\"result.json\", \"w\") as f:\n",
        "    json.dump(payload, f)\n",
        "\n",
        "# Download + base64 for Airflow\n",
        "files.download(\"result.json\")\n",
        "!base64 result.json > result.b64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed32629a",
        "outputId": "e52ac309-7936-476c-be0c-0fb158a703f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded into MongoDB collection 'fact_inventory'\n"
          ]
        }
      ],
      "source": [
        "# Load data into MongoDB\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client.shelfsense\n",
        "inventory_collection = db.fact_inventory\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame (if necessary for your pymongo version and data size)\n",
        "# For large datasets, consider writing directly from Spark to MongoDB if you have the connector\n",
        "\n",
        "# Example of converting to Pandas and inserting (may be slow for large data)\n",
        "# inventory_pandas_df = fact_inventory.toPandas()\n",
        "# inventory_collection.insert_many(inventory_pandas_df.to_dict('records'))\n",
        "\n",
        "# Alternative: Write directly from Spark (requires spark-mongodb connector)\n",
        "# fact_inventory.write.format(\"mongo\").mode(\"append\").option(\"uri\", MONGO_URI).option(\"database\", \"shelfsense\").option(\"collection\", \"fact_inventory\").save()\n",
        "\n",
        "print(\"Data loaded into MongoDB collection 'fact_inventory'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AKdEdZeP3Sw"
      },
      "outputs": [],
      "source": [
        "payload = {\n",
        "    \"fact\": fact_inventory.toPandas().to_dict('records'),\n",
        "    \"optimized\": grouped.to_dict('records')  # From PuLP cell\n",
        "}\n",
        "with open('result.json', 'w') as f:\n",
        "    json.dump(payload, f)\n",
        "print(\"result.json created for Airflow\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
