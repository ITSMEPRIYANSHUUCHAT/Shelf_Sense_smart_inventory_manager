{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.3-bin-hadoop3.tgz\n",
        "!pip install -q pyspark pymongo python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CAGpti6-jCN",
        "outputId": "9d9f8c51-62e6-45bf-8a83-ebd0748c4412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "809491e3"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\"\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date\n",
        "from pymongo import MongoClient\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"ShelfTransform\").config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
        "\n",
        "# Add your .env keys manually here (don't save in notebook)\n",
        "MONGO_URI = \"mongodb+srv://priyanshu:Piyu%40best123@cluster01.mmq51.mongodb.net/shelfsense?retryWrites=true&w=majority\"  # Paste from .env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "sales_df = spark.read.csv('sales_perishables.csv', header=True, inferSchema=True)\n",
        "inventory_df = spark.read.csv('inventory_chunk_*.csv', header=True, inferSchema=True)  # Wildcard for chunks\n",
        "weather_df = spark.read.csv('weather_daily_clean.csv', header=True, inferSchema=True)\n",
        "promo_df = spark.read.csv('promotions_weekly.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Clean dates (adjust formats if errors)\n",
        "sales_df = sales_df.withColumn('Date_Received', to_date(col('Date_Received'), 'MM/dd/yyyy'))\n",
        "inventory_df = inventory_df.withColumn('Date', to_date(col('Date'), 'yyyy-MM-dd'))\n",
        "weather_df = weather_df.withColumn('date', to_date(col('date'), 'yyyy-MM-dd'))"
      ],
      "metadata": {
        "id": "XMpVT8VpCVR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20be6a78",
        "outputId": "b6d18230-cb8e-4b8f-faee-144ce09977c0"
      },
      "source": [
        "# 1. Fix column names\n",
        "inventory_df = inventory_df.withColumnRenamed(\"Product ID\", \"Product_ID\") \\\n",
        "                           .withColumnRenamed(\"Date\", \"Date_Received\")\n",
        "\n",
        "# 2. Use a REAL common key (use row number as dummy key for demo)\n",
        "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create row_id on each DataFrame before joining\n",
        "sales_df_with_id = sales_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"Product_ID\")))\n",
        "inventory_df_with_id = inventory_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"Product_ID\")))\n",
        "weather_df_with_id = weather_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"date\")))\n",
        "\n",
        "\n",
        "# 3. Re-run your join\n",
        "joined_df = (sales_df_with_id.join(inventory_df_with_id, \"row_id\", 'inner')\n",
        "                     .join(weather_df_with_id, \"row_id\", 'left')\n",
        "                     .select(sales_df_with_id['Product_ID'], sales_df_with_id['Category'], col('Stock_Quantity'), col('Units Sold').alias('Units_Sold'),\n",
        "                             col('meantemp'), col('humidity'), sales_df_with_id['Date_Received']))\n",
        "\n",
        "fact_inventory = joined_df.select('Product_ID', 'Date_Received', 'Stock_Quantity',\n",
        "                                  'Units_Sold', col('meantemp').alias('Weather_Temp'),\n",
        "                                  col('humidity').alias('Weather_Humidity'))\n",
        "\n",
        "fact_inventory.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------+----------+-----------------+-----------------+\n",
            "| Product_ID|Date_Received|Stock_Quantity|Units_Sold|     Weather_Temp| Weather_Humidity|\n",
            "+-----------+-------------+--------------+----------+-----------------+-----------------+\n",
            "|00-119-8780|   2025-02-17|            40|        73|             10.0|             84.5|\n",
            "|00-215-7434|   2024-04-28|            14|       178|              7.4|             92.0|\n",
            "|00-366-9496|   2024-08-09|            30|       203|7.166666666666667|             87.0|\n",
            "|00-405-7428|   2025-01-20|            81|       145|8.666666666666666|71.33333333333333|\n",
            "|00-440-9568|   2024-09-23|            53|       248|              6.0|86.83333333333333|\n",
            "+-----------+-------------+--------------+----------+-----------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Mock prediction: Add 'predicted_waste_risk' column (high if temp > 25 and humidity > 80)\n",
        "fact_inventory = fact_inventory.withColumn(\n",
        "    \"predicted_waste_risk\",\n",
        "    when((col(\"Weather_Temp\") > 25) & (col(\"Weather_Humidity\") > 80), \"High\")\n",
        "    .when((col(\"Weather_Temp\") > 20) & (col(\"Weather_Humidity\") > 70), \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "fact_inventory.show(5)  # Preview with new column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUbeyzuPmeWX",
        "outputId": "2f6e0a92-8460-4285-9b03-51a4ce65f497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------+----------+-----------------+-----------------+--------------------+\n",
            "| Product_ID|Date_Received|Stock_Quantity|Units_Sold|     Weather_Temp| Weather_Humidity|predicted_waste_risk|\n",
            "+-----------+-------------+--------------+----------+-----------------+-----------------+--------------------+\n",
            "|00-119-8780|   2025-02-17|            40|        73|             10.0|             84.5|                 Low|\n",
            "|00-215-7434|   2024-04-28|            14|       178|              7.4|             92.0|                 Low|\n",
            "|00-366-9496|   2024-08-09|            30|       203|7.166666666666667|             87.0|                 Low|\n",
            "|00-405-7428|   2025-01-20|            81|       145|8.666666666666666|71.33333333333333|                 Low|\n",
            "|00-440-9568|   2024-09-23|            53|       248|              6.0|86.83333333333333|                 Low|\n",
            "+-----------+-------------+--------------+----------+-----------------+-----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pulp\n",
        "\n",
        "# Convert to Pandas for PuLP (small data OK)\n",
        "fact_pd = fact_inventory.toPandas()\n",
        "\n",
        "# Group by Product_ID for averages\n",
        "grouped = fact_pd.groupby('Product_ID').agg({\n",
        "    'Stock_Quantity': 'mean',\n",
        "    'Units_Sold': 'mean',\n",
        "    'Weather_Temp': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# PuLP model: Minimize cost (reorder + waste risk proxy)\n",
        "prob = pulp.LpProblem(\"ReorderOptWithPrediction\", pulp.LpMinimize)\n",
        "\n",
        "products = grouped['Product_ID'].tolist()\n",
        "reorder_qty = pulp.LpVariable.dicts(\"Reorder\", products, lowBound=0, cat='Integer')\n",
        "\n",
        "# Objective: Min reorders weighted by temp risk (higher temp = higher \"waste cost\")\n",
        "prob += pulp.lpSum([reorder_qty[p] * grouped[grouped['Product_ID'] == p]['Weather_Temp'].values[0] for p in products])\n",
        "\n",
        "# Constraints: Stock + reorder >= demand (Units_Sold)\n",
        "for p in products:\n",
        "    row = grouped[grouped['Product_ID'] == p]\n",
        "    current_stock = row['Stock_Quantity'].values[0]\n",
        "    demand = row['Units_Sold'].values[0]\n",
        "    prob += reorder_qty[p] + current_stock >= demand, f\"Demand_{p}\"\n",
        "\n",
        "prob.solve()\n",
        "\n",
        "# Add results to table\n",
        "reorders = {p: pulp.value(reorder_qty[p]) for p in products}\n",
        "grouped['recommended_reorder'] = grouped['Product_ID'].map(reorders)\n",
        "grouped.to_csv('optimized_reorders.csv', index=False)  # Download this\n",
        "\n",
        "print(\"Optimized table:\")\n",
        "print(grouped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZSVMc4jnLgP",
        "outputId": "30dc1307-f326-4a6d-ec98-499ad10469f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized table:\n",
            "      Product_ID  Stock_Quantity  Units_Sold  Weather_Temp  \\\n",
            "0    00-119-8780            40.0        73.0     10.000000   \n",
            "1    00-215-7434            14.0       178.0      7.400000   \n",
            "2    00-366-9496            30.0       203.0      7.166667   \n",
            "3    00-405-7428            81.0       145.0      8.666667   \n",
            "4    00-440-9568            53.0       248.0      6.000000   \n",
            "..           ...             ...         ...           ...   \n",
            "596  99-048-8310            32.0        56.0     33.250000   \n",
            "597  99-194-5600            57.0        14.0     32.500000   \n",
            "598  99-543-5039            65.0       295.0     33.000000   \n",
            "599  99-561-4871            17.0        66.0     33.125000   \n",
            "600  99-961-0767            85.0       128.0     33.125000   \n",
            "\n",
            "     recommended_reorder  \n",
            "0                   33.0  \n",
            "1                  164.0  \n",
            "2                  173.0  \n",
            "3                   64.0  \n",
            "4                  195.0  \n",
            "..                   ...  \n",
            "596                 24.0  \n",
            "597                  0.0  \n",
            "598                230.0  \n",
            "599                 49.0  \n",
            "600                 43.0  \n",
            "\n",
            "[601 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === COLAB ONLY — RUN ONCE ===\n",
        "import json, base64, os\n",
        "from google.colab import files\n",
        "from pyspark.sql.functions import date_format\n",
        "\n",
        "# Save outputs\n",
        "# Convert date column to string before converting to Pandas for Parquet\n",
        "fact_inventory.withColumn(\"Date_Received\", date_format(\"Date_Received\", \"yyyy-MM-dd\")).toPandas().to_parquet(\"fact_inventory.parquet\", index=False)\n",
        "\n",
        "optimized = grouped  # from your PuLP cell\n",
        "optimized.to_parquet(\"optimized_reorders.parquet\", index=False)\n",
        "\n",
        "# Create a single JSON with both\n",
        "# Convert date column to string in fact_inventory_for_json\n",
        "fact_inventory_for_json = fact_inventory.withColumn(\"Date_Received\", date_format(\"Date_Received\", \"yyyy-MM-dd\")).toPandas()\n",
        "optimized_for_json = optimized.to_dict('records')\n",
        "\n",
        "\n",
        "payload = {\n",
        "    \"fact\": fact_inventory_for_json.to_dict('records'),\n",
        "    \"optimized\": optimized_for_json\n",
        "}\n",
        "with open(\"result.json\", \"w\") as f:\n",
        "    json.dump(payload, f)\n",
        "\n",
        "# Download + base64 for Airflow\n",
        "files.download(\"result.json\")\n",
        "!base64 result.json > result.b64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qoro09cbotGp",
        "outputId": "0495e9e4-0e78-4cfc-a376-ef09107198c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6aa81a60-ad02-4334-808b-a6104f4b210e\", \"result.json\", 194178)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed32629a",
        "outputId": "e52ac309-7936-476c-be0c-0fb158a703f8"
      },
      "source": [
        "# Load data into MongoDB\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client.shelfsense\n",
        "inventory_collection = db.fact_inventory\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame (if necessary for your pymongo version and data size)\n",
        "# For large datasets, consider writing directly from Spark to MongoDB if you have the connector\n",
        "\n",
        "# Example of converting to Pandas and inserting (may be slow for large data)\n",
        "# inventory_pandas_df = fact_inventory.toPandas()\n",
        "# inventory_collection.insert_many(inventory_pandas_df.to_dict('records'))\n",
        "\n",
        "# Alternative: Write directly from Spark (requires spark-mongodb connector)\n",
        "# fact_inventory.write.format(\"mongo\").mode(\"append\").option(\"uri\", MONGO_URI).option(\"database\", \"shelfsense\").option(\"collection\", \"fact_inventory\").save()\n",
        "\n",
        "print(\"Data loaded into MongoDB collection 'fact_inventory'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded into MongoDB collection 'fact_inventory'\n"
          ]
        }
      ]
    }
  ]
}