{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CAGpti6-jCN",
        "outputId": "e798b2e1-3e90-48eb-f47e-999c9d370e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/331.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.3-bin-hadoop3.tgz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exVV-C-a1ODR"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark pymongo python-dotenv pulp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "809491e3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\"\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, when, row_number, date_format\n",
        "from pyspark.sql.window import Window\n",
        "from pymongo import MongoClient\n",
        "from urllib.parse import quote_plus\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"ShelfTransform\").config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
        "\n",
        "# Add your .env keys manually here (don't save in notebook)\n",
        "MONGO_URI = \"mongodb+srv://priyanshu:PiyuMax123@cluster01.mmq51.mongodb.net/shelfsensestorage?retryWrites=true&w=majority\" # Paste from .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4VEEpZTx1U7",
        "outputId": "6fde66ea-9d64-4fc0-987c-5604d9a97d48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = '/content/drive/My Drive/ShelfData'\n",
        "# Load data\n",
        "sales_df = spark.read.csv(f'{DRIVE_PATH}/sales_perishables.csv', header=True, inferSchema=True)\n",
        "inventory_df = spark.read.csv(f'{DRIVE_PATH}/inventory_chunk_*.csv', header=True, inferSchema=True) # Wildcard for chunks\n",
        "weather_df = spark.read.csv(f'{DRIVE_PATH}/weather_daily_clean.csv', header=True, inferSchema=True)  # FIXED: header=True\n",
        "promo_df = spark.read.csv(f'{DRIVE_PATH}/promotions_weekly.csv', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMpVT8VpCVR_"
      },
      "outputs": [],
      "source": [
        "# Clean dates (adjust formats if errors)\n",
        "sales_df = sales_df.withColumn('Date_Received', to_date(col('Date_Received'), 'MM/dd/yyyy'))\n",
        "inventory_df = inventory_df.withColumn('Date', to_date(col('Date'), 'yyyy-MM-dd'))\n",
        "weather_df = weather_df.withColumn('date', to_date(col('date'), 'yyyy-MM-dd'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20be6a78",
        "outputId": "9beabc61-cca5-4e44-eae8-beec8927724b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------+--------------+----------+-----------------+-----------------+\n",
            "| Product_ID|Date_Received|Stock_Quantity|Units_Sold|     Weather_Temp| Weather_Humidity|\n",
            "+-----------+-------------+--------------+----------+-----------------+-----------------+\n",
            "|00-119-8780|   2025-02-17|            40|        73|             10.0|             84.5|\n",
            "|00-215-7434|   2024-04-28|            14|       178|              7.4|             92.0|\n",
            "|00-366-9496|   2024-08-09|            30|       203|7.166666666666667|             87.0|\n",
            "|00-405-7428|   2025-01-20|            81|       145|8.666666666666666|71.33333333333333|\n",
            "|00-440-9568|   2024-09-23|            53|       248|              6.0|86.83333333333333|\n",
            "+-----------+-------------+--------------+----------+-----------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Fix column names\n",
        "inventory_df = inventory_df.withColumnRenamed(\"Product ID\", \"Product_ID\") \\\n",
        "                           .withColumnRenamed(\"Date\", \"Date_Received\")\n",
        "\n",
        "# 2. Use a REAL common key (use row number as dummy key for demo)\n",
        "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create row_id on each DataFrame before joining\n",
        "sales_df_with_id = sales_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"Product_ID\")))\n",
        "inventory_df_with_id = inventory_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"Product_ID\")))\n",
        "weather_df_with_id = weather_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"date\")))\n",
        "\n",
        "\n",
        "# 3. Re-run your join\n",
        "joined_df = (sales_df_with_id.join(inventory_df_with_id, \"row_id\", 'inner')\n",
        "                     .join(weather_df_with_id, \"row_id\", 'left')\n",
        "                     .select(sales_df_with_id['Product_ID'], sales_df_with_id['Category'], col('Stock_Quantity'), col('Units Sold').alias('Units_Sold'),\n",
        "                             col('meantemp'), col('humidity'), sales_df_with_id['Date_Received']))\n",
        "\n",
        "fact_inventory = joined_df.select('Product_ID', 'Date_Received', 'Stock_Quantity',\n",
        "                                  'Units_Sold', col('meantemp').alias('Weather_Temp'),\n",
        "                                  col('humidity').alias('Weather_Humidity'))\n",
        "\n",
        "fact_inventory.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUbeyzuPmeWX",
        "outputId": "ae42d99d-ab41-472a-b8ee-41ee4022861f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------+--------------+----------+-----------------+-----------------+--------------------+\n",
            "| Product_ID|Date_Received|Stock_Quantity|Units_Sold|     Weather_Temp| Weather_Humidity|predicted_waste_risk|\n",
            "+-----------+-------------+--------------+----------+-----------------+-----------------+--------------------+\n",
            "|00-119-8780|   2025-02-17|            40|        73|             10.0|             84.5|                 Low|\n",
            "|00-215-7434|   2024-04-28|            14|       178|              7.4|             92.0|                 Low|\n",
            "|00-366-9496|   2024-08-09|            30|       203|7.166666666666667|             87.0|                 Low|\n",
            "|00-405-7428|   2025-01-20|            81|       145|8.666666666666666|71.33333333333333|                 Low|\n",
            "|00-440-9568|   2024-09-23|            53|       248|              6.0|86.83333333333333|                 Low|\n",
            "+-----------+-------------+--------------+----------+-----------------+-----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Mock prediction: Add 'predicted_waste_risk' column (high if temp > 25 and humidity > 80)\n",
        "fact_inventory = fact_inventory.withColumn(\n",
        "    \"predicted_waste_risk\",\n",
        "    when((col(\"Weather_Temp\") > 25) & (col(\"Weather_Humidity\") > 80), \"High\")\n",
        "    .when((col(\"Weather_Temp\") > 20) & (col(\"Weather_Humidity\") > 70), \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "fact_inventory.show(5)  # Preview with new column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZSVMc4jnLgP",
        "outputId": "222f3031-3c78-4b5e-e43b-d14d899bba94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pulp\n",
            "  Downloading pulp-3.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Downloading pulp-3.3.0-py3-none-any.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-3.3.0\n",
            "Optimized table:\n",
            "      Product_ID  Stock_Quantity  Units_Sold  Weather_Temp  \\\n",
            "0    00-119-8780            40.0        73.0     10.000000   \n",
            "1    00-215-7434            14.0       178.0      7.400000   \n",
            "2    00-366-9496            30.0       203.0      7.166667   \n",
            "3    00-405-7428            81.0       145.0      8.666667   \n",
            "4    00-440-9568            53.0       248.0      6.000000   \n",
            "..           ...             ...         ...           ...   \n",
            "596  99-048-8310            32.0        56.0     33.250000   \n",
            "597  99-194-5600            57.0        14.0     32.500000   \n",
            "598  99-543-5039            65.0       295.0     33.000000   \n",
            "599  99-561-4871            17.0        66.0     33.125000   \n",
            "600  99-961-0767            85.0       128.0     33.125000   \n",
            "\n",
            "     recommended_reorder  \n",
            "0                   33.0  \n",
            "1                  164.0  \n",
            "2                  173.0  \n",
            "3                   64.0  \n",
            "4                  195.0  \n",
            "..                   ...  \n",
            "596                 24.0  \n",
            "597                  0.0  \n",
            "598                230.0  \n",
            "599                 49.0  \n",
            "600                 43.0  \n",
            "\n",
            "[601 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "!pip install pulp\n",
        "import pulp\n",
        "# Convert to Pandas for PuLP (small data OK)\n",
        "fact_pd = fact_inventory.toPandas()\n",
        "# Group by Product_ID for averages\n",
        "grouped = fact_pd.groupby('Product_ID').agg({\n",
        "    'Stock_Quantity': 'mean',\n",
        "    'Units_Sold': 'mean',\n",
        "    'Weather_Temp': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# PuLP model: Minimize cost (reorder + waste risk proxy)\n",
        "prob = pulp.LpProblem(\"ReorderOptWithPrediction\", pulp.LpMinimize)\n",
        "products = grouped['Product_ID'].tolist()\n",
        "reorder_qty = pulp.LpVariable.dicts(\"Reorder\", products, lowBound=0, cat='Integer')\n",
        "\n",
        "# Objective: Min reorders weighted by temp risk (higher temp = higher \"waste cost\")\n",
        "prob += pulp.lpSum([reorder_qty[p] * grouped[grouped['Product_ID'] == p]['Weather_Temp'].values[0] for p in products])\n",
        "\n",
        "# Constraints: Stock + reorder >= demand (Units_Sold)\n",
        "for p in products:\n",
        "    row = grouped[grouped['Product_ID'] == p]\n",
        "    current_stock = row['Stock_Quantity'].values[0]  # FIXED: current_stock, not current_stock\n",
        "    demand = row['Units_Sold'].values[0]\n",
        "    prob += reorder_qty[p] + current_stock >= demand, f\"Demand_{p}\"\n",
        "\n",
        "prob.solve()\n",
        "\n",
        "# Add results to table\n",
        "reorders = {p: pulp.value(reorder_qty[p]) for p in products}\n",
        "grouped['recommended_reorder'] = grouped['Product_ID'].map(reorders)\n",
        "grouped.to_csv('optimized_reorders.csv', index=False) # Download this\n",
        "\n",
        "print(\"Optimized table:\")\n",
        "print(grouped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoro09cbotGp",
        "outputId": "53b17a14-7c6c-42e6-aceb-65d5aa6f17cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result.json created for Airflow (dates as strings)\n",
            "Loaded to MongoDB for testing\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import date_format\n",
        "\n",
        "# Convert date to string for JSON (before toPandas)\n",
        "fact_inventory_json = fact_inventory.withColumn(\"Date_Received\", date_format(\"Date_Received\", \"yyyy-MM-dd\")).toPandas()\n",
        "\n",
        "# Payload with string dates\n",
        "payload = {\n",
        "    \"fact\": fact_inventory_json.to_dict('records'),\n",
        "    \"dim_products\": sales_df.select('Product_ID', 'Product_Name', 'Category', 'Unit_Price').distinct().toPandas().to_dict('records'),\n",
        "    \"dim_dates\": fact_inventory.select('Date_Received').distinct().withColumnRenamed('Date_Received', 'Date').withColumn(\"Date\", date_format(\"Date\", \"yyyy-MM-dd\")).toPandas().to_dict('records'),\n",
        "    \"optimized\": grouped.to_dict('records')\n",
        "}\n",
        "import json\n",
        "with open('result.json', 'w') as f:\n",
        "    json.dump(payload, f)\n",
        "print(\"result.json created for Airflow (dates as strings)\")\n",
        "\n",
        "# Optional MongoDB Load for testing\n",
        "MONGO_URI = \"mongodb+srv://priyanshu:PiyuMax123@cluster01.mmq51.mongodb.net/shelfsense?retryWrites=true&w=majority\"\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client['shelfsense']  # FIXED: dict access\n",
        "db['fact_inventory'].insert_many(fact_inventory_json.to_dict('records'))\n",
        "print(\"Loaded to MongoDB for testing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed32629a",
        "outputId": "72f4a8f2-ea11-481a-854b-0a0a4a41cc0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded into MongoDB collection 'fact_inventory'\n"
          ]
        }
      ],
      "source": [
        "# Load data into MongoDB\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client.shelfsense\n",
        "inventory_collection = db.fact_inventory\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame (if necessary for your pymongo version and data size)\n",
        "# For large datasets, consider writing directly from Spark to MongoDB if you have the connector\n",
        "\n",
        "# Example of converting to Pandas and inserting (may be slow for large data)\n",
        "# inventory_pandas_df = fact_inventory.toPandas()\n",
        "# inventory_collection.insert_many(inventory_pandas_df.to_dict('records'))\n",
        "\n",
        "# Alternative: Write directly from Spark (requires spark-mongodb connector)\n",
        "# fact_inventory.write.format(\"mongo\").mode(\"append\").option(\"uri\", MONGO_URI).option(\"database\", \"shelfsense\").option(\"collection\", \"fact_inventory\").save()\n",
        "\n",
        "print(\"Data loaded into MongoDB collection 'fact_inventory'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
