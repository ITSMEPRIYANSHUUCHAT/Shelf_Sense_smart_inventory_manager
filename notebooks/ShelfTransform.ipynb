{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CAGpti6-jCN",
        "outputId": "e798b2e1-3e90-48eb-f47e-999c9d370e19"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.3-bin-hadoop3.tgz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exVV-C-a1ODR"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark pymongo python-dotenv pulp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "809491e3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\"\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, when, row_number, date_format\n",
        "from pyspark.sql.window import Window\n",
        "from pymongo import MongoClient\n",
        "from urllib.parse import quote_plus\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"ShelfTransform\").config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
        "\n",
        "# Add your .env keys manually here (don't save in notebook)\n",
        "MONGO_URI = \"mongodb+srv://priyanshu:PiyuMax123@cluster01.mmq51.mongodb.net/shelfsensestorage?retryWrites=true&w=majority\" # Paste from .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4VEEpZTx1U7",
        "outputId": "6fde66ea-9d64-4fc0-987c-5604d9a97d48"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = '/content/drive/My Drive/ShelfData'\n",
        "# Load data\n",
        "sales_df = spark.read.csv(f'{DRIVE_PATH}/sales_perishables.csv', header=True, inferSchema=True)\n",
        "inventory_df = spark.read.csv(f'{DRIVE_PATH}/inventory_chunk_*.csv', header=True, inferSchema=True) # Wildcard for chunks\n",
        "weather_df = spark.read.csv(f'{DRIVE_PATH}/weather_daily_clean.csv', header=True, inferSchema=True)\n",
        "promo_df = spark.read.csv(f'{DRIVE_PATH}/promotions_weekly.csv', header=True, inferSchema=True)  # FIXED: header=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMpVT8VpCVR_"
      },
      "outputs": [],
      "source": [
        "# Clean dates (adjust formats if errors)\n",
        "sales_df = sales_df.withColumn('Date_Received', to_date(col('Date_Received'), 'MM/dd/yyyy'))\n",
        "inventory_df = inventory_df.withColumn('Date', to_date(col('Date'), 'yyyy-MM-dd'))\n",
        "weather_df = weather_df.withColumn('date', to_date(col('date'), 'yyyy-MM-dd'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20be6a78",
        "outputId": "9beabc61-cca5-4e44-eae8-beec8927724b"
      },
      "outputs": [],
      "source": [
        "# 1. Fix column names\n",
        "inventory_df = inventory_df.withColumnRenamed(\"Product ID\", \"Product_ID\") \\\n",
        "                           .withColumnRenamed(\"Date\", \"Date_Received\")\n",
        "\n",
        "# 2. Use a REAL common key (use row number as dummy key for demo)\n",
        "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create row_id on each DataFrame before joining\n",
        "sales_df_with_id = sales_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"Product_ID\")))\n",
        "inventory_df_with_id = inventory_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"Product_ID\")))\n",
        "weather_df_with_id = weather_df.withColumn(\"row_id\", row_number().over(Window.orderBy(\"date\")))\n",
        "\n",
        "\n",
        "# 3. Re-run your join\n",
        "joined_df = (sales_df_with_id.join(inventory_df_with_id, \"row_id\", 'inner')\n",
        "                     .join(weather_df_with_id, \"row_id\", 'left')\n",
        "                     .select(sales_df_with_id['Product_ID'], sales_df_with_id['Category'], col('Stock_Quantity'), col('Units Sold').alias('Units_Sold'),\n",
        "                             col('meantemp'), col('humidity'), sales_df_with_id['Date_Received']))\n",
        "\n",
        "fact_inventory = joined_df.select('Product_ID', 'Date_Received', 'Stock_Quantity',\n",
        "                                  'Units_Sold', col('meantemp').alias('Weather_Temp'),\n",
        "                                  col('humidity').alias('Weather_Humidity'))\n",
        "\n",
        "fact_inventory.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUbeyzuPmeWX",
        "outputId": "ae42d99d-ab41-472a-b8ee-41ee4022861f"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Mock prediction: Add 'predicted_waste_risk' column (high if temp > 25 and humidity > 80)\n",
        "fact_inventory = fact_inventory.withColumn(\n",
        "    \"predicted_waste_risk\",\n",
        "    when((col(\"Weather_Temp\") > 25) & (col(\"Weather_Humidity\") > 80), \"High\")\n",
        "    .when((col(\"Weather_Temp\") > 20) & (col(\"Weather_Humidity\") > 70), \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "fact_inventory.show(5)  # Preview with new column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoro09cbotGp",
        "outputId": "53b17a14-7c6c-42e6-aceb-65d5aa6f17cc"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import date_format\n",
        "# Convert date to string for JSON (before toPandas)\n",
        "fact_inventory_json = fact_inventory.withColumn(\"Date_Received\", date_format(\"Date_Received\", \"yyyy-MM-dd\")).toPandas()\n",
        "\n",
        "# Payload with string dates\n",
        "payload = {\n",
        "    \"fact\": fact_inventory_json.to_dict('records'),\n",
        "    \"dim_products\": sales_df.select('Product_ID', 'Product_Name', 'Category', 'Unit_Price').distinct().toPandas().to_dict('records'),\n",
        "    \"dim_dates\": fact_inventory.select('Date_Received').distinct().withColumnRenamed('Date_Received', 'Date').withColumn(\"Date\", date_format(\"Date\", \"yyyy-MM-dd\")).toPandas().to_dict('records'),\n",
        "    \"optimized\": grouped.to_dict('records')\n",
        "}\n",
        "import json\n",
        "with open('result.json', 'w') as f:\n",
        "    json.dump(payload, f)\n",
        "print(\"result.json created for Airflow (dates as strings)\")\n",
        "\n",
        "# Optional MongoDB Load for testing\n",
        "MONGO_URI = \"mongodb+srv://priyanshu:PiyuMax123@cluster01.mmq51.mongodb.net/shelfsense?retryWrites=true&w=majority\"\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client['shelfsense']  # FIXED: dict access\n",
        "db['fact_inventory'].insert_many(fact_inventory_json.to_dict('records'))\n",
        "print(\"Loaded to MongoDB for testing\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
